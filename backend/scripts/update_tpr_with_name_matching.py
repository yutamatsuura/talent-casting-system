#!/usr/bin/env python3
"""
TPRãƒ‘ãƒ¯ãƒ¼ã‚¹ã‚³ã‚¢æ›´æ–°ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆåå‰ãƒãƒƒãƒãƒ³ã‚°å¯¾å¿œç‰ˆï¼‰

æ©Ÿèƒ½:
- CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Gåˆ—ã®ãƒ‘ãƒ¯ãƒ¼ã‚¹ã‚³ã‚¢ã‚’å–ã‚Šè¾¼ã¿
- ã‚¿ãƒ¬ãƒ³ãƒˆåã§ã®ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†
- ãƒãƒƒãƒãƒ³ã‚°å¤±æ•—æ™‚ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- base_power_scoreã®è‡ªå‹•æ›´æ–°
- ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³æ©Ÿèƒ½

ä½¿ç”¨æ–¹æ³•:
    python update_tpr_with_name_matching.py --dry-run
    python update_tpr_with_name_matching.py --execute
"""

import argparse
import asyncio
import logging
import pandas as pd
import sys
from pathlib import Path
from datetime import datetime
from decimal import Decimal
from sqlalchemy import text, select, func
from difflib import SequenceMatcher

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))
from app.db.connection import init_db, get_session_maker
from app.models import TalentScore, Talent

# ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from scripts.talent_name_mapping_dictionary import MANUAL_NAME_MAPPING, get_manual_mapping, get_alternative_names
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¾æ›¸ï¼ˆãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼‰
    MANUAL_NAME_MAPPING = {
        "ã‚¤ãƒãƒ­ãƒ¼": "éˆ´æœ¨ä¸€æœ—ï¼ˆã‚¤ãƒãƒ­ãƒ¼ï¼‰",
        "ãƒ’ã‚«ã‚­ãƒ³": "HIKAKIN",
    }
    def get_manual_mapping(csv_name: str) -> str:
        return MANUAL_NAME_MAPPING.get(csv_name)
    def get_alternative_names(csv_name: str) -> list:
        return []

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(f'tpr_update_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# TPRãƒ•ã‚¡ã‚¤ãƒ«ã¨å¯¾å¿œã™ã‚‹ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤IDï¼ˆä¿®æ­£ç‰ˆï¼šè¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã«åˆã‚ã›ã¦9-16ã‚’ä½¿ç”¨ï¼‰
TPR_FILES_MAPPING = {
    "TPR_ç”·æ€§12ï½19_202508.csv": 9,   # ä¿®æ­£: 13 â†’ 9 (ç”·æ€§12-19æ­³)
    "TPR_å¥³æ€§12ï½19_202508.csv": 10,  # ä¿®æ­£: 9 â†’ 10 (å¥³æ€§12-19æ­³)
    "TPR_ç”·æ€§20ï½34_202508.csv": 11,  # ä¿®æ­£: 14 â†’ 11 (ç”·æ€§20-34æ­³)
    "TPR_å¥³æ€§20ï½34_202508.csv": 12,  # ä¿®æ­£: 10 â†’ 12 (å¥³æ€§20-34æ­³)
    "TPR_ç”·æ€§35ï½49_202508.csv": 13,  # ä¿®æ­£: 15 â†’ 13 (ç”·æ€§35-49æ­³)
    "TPR_å¥³æ€§35ï½49_202508.csv": 14,  # ä¿®æ­£: 11 â†’ 14 (å¥³æ€§35-49æ­³)
    "TPR_ç”·æ€§50ï½69_202508.csv": 15,  # ä¿®æ­£: 16 â†’ 15 (ç”·æ€§50-69æ­³)
    "TPR_å¥³æ€§50ï½69_202508.csv": 16,  # ä¿®æ­£: 12 â†’ 16 (å¥³æ€§50-69æ­³)
}

# CSVãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
CSV_DIR = Path("/Users/lennon/projects/talent-casting-form/DBdata/ã€TPRã€‘Gåˆ—ã®ãƒ‘ãƒ¯ãƒ¼ã‚¹ã‚³ã‚¢ã‚’æ¡ç”¨ã™ã‚‹æƒ³å®šã§ã™")


class TPRImporter:
    def __init__(self):
        self.talent_map = {}
        self.matched_count = 0
        self.unmatched_count = 0
        self.unmatched_list = []
        self.fuzzy_matches = []

    async def load_talent_mapping(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚¿ãƒ¬ãƒ³ãƒˆåãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“‹ Loading talent name mapping from database...")

        async with get_session_maker()() as session:
            result = await session.execute(
                select(Talent.account_id, Talent.name_full_for_matching)
                .where(Talent.del_flag == 0)
            )

            for row in result.all():
                if row.name_full_for_matching:
                    # å®Œå…¨ä¸€è‡´ç”¨
                    self.talent_map[row.name_full_for_matching.strip()] = row.account_id

        logger.info(f"âœ… Loaded {len(self.talent_map)} talent names")

    def normalize_name(self, name):
        """åå‰ã®æ­£è¦åŒ–ï¼ˆãƒãƒƒãƒãƒ³ã‚°ç²¾åº¦å‘ä¸Šã®ãŸã‚ï¼‰"""
        import re

        # åŸºæœ¬æ­£è¦åŒ–
        normalized = name.strip()

        # æ‹¬å¼§ã¨ãã®ä¸­èº«ã‚’é™¤å»ï¼ˆã‚³ãƒ³ãƒ“åå¯¾å¿œï¼‰
        normalized = re.sub(r'[ï¼ˆ(][^ï¼‰)]*[ï¼‰)]', '', normalized)

        # ã‚¹ãƒšãƒ¼ã‚¹ãƒ»ãƒ”ãƒªã‚ªãƒ‰ãƒ»è¨˜å·ã‚’é™¤å»
        normalized = re.sub(r'[.\sã€€ãƒ»!?]', '', normalized)

        # å…¨è§’è‹±æ•°ã‚’åŠè§’ã«å¤‰æ›
        normalized = normalized.translate(str.maketrans(
            'ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™',
            'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'
        ))

        # å¤§æ–‡å­—ã«çµ±ä¸€ï¼ˆè‹±å­—ã®å ´åˆï¼‰
        normalized = normalized.upper()

        return normalized

    def find_best_match(self, csv_name, threshold=0.75):
        """æ”¹å–„ã•ã‚ŒãŸã‚ã„ã¾ã„ãƒãƒƒãƒãƒ³ã‚°ï¼ˆãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸çµ±åˆç‰ˆï¼‰"""
        csv_name = csv_name.strip()

        # 0. æ‰‹å‹•ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‚’æœ€å„ªå…ˆã§ãƒã‚§ãƒƒã‚¯
        manual_mapping = get_manual_mapping(csv_name)
        if manual_mapping and manual_mapping in self.talent_map:
            return self.talent_map[manual_mapping], "manual_mapping"

        # 1. å®Œå…¨ä¸€è‡´ã‚’æœ€å„ªå…ˆ
        if csv_name in self.talent_map:
            return self.talent_map[csv_name], "exact"

        # 2. ä»£æ›¿å€™è£œåã§ã®ãƒãƒƒãƒãƒ³ã‚°ï¼ˆã‚«ãƒŠãƒ»è‹±å­—å¤‰æ›ãªã©ï¼‰
        alternative_names = get_alternative_names(csv_name)
        for alt_name in alternative_names:
            if alt_name in self.talent_map:
                return self.talent_map[alt_name], "alternative_mapping"

        # 3. æ­£è¦åŒ–åã§ã®å®Œå…¨ä¸€è‡´
        normalized_csv = self.normalize_name(csv_name)
        for db_name in self.talent_map.keys():
            if normalized_csv == self.normalize_name(db_name):
                return self.talent_map[db_name], "normalized_exact"

        # 4. æ‹¬å¼§éƒ¨åˆ†ã®ã¿ã§ãƒãƒƒãƒãƒ³ã‚°ï¼ˆã‚³ãƒ³ãƒ“åãªã©ï¼‰
        import re
        bracket_match = re.search(r'[ï¼ˆ(]([^ï¼‰)]+)[ï¼‰)]', csv_name)
        if bracket_match:
            bracket_content = bracket_match.group(1)
            for db_name in self.talent_map.keys():
                if bracket_content in db_name or self.normalize_name(bracket_content) == self.normalize_name(db_name):
                    return self.talent_map[db_name], "bracket_match"

        # 5. æ‹¬å¼§ã‚’é™¤å»ã—ãŸéƒ¨åˆ†ã§ãƒãƒƒãƒãƒ³ã‚°ï¼ˆå€‹äººåï¼‰
        name_without_bracket = re.sub(r'[ï¼ˆ(][^ï¼‰)]*[ï¼‰)]', '', csv_name).strip()
        if name_without_bracket != csv_name:
            for db_name in self.talent_map.keys():
                if name_without_bracket == db_name or self.normalize_name(name_without_bracket) == self.normalize_name(db_name):
                    return self.talent_map[db_name], "individual_match"

        # 6. ã‚ã„ã¾ã„ãƒãƒƒãƒãƒ³ã‚°ï¼ˆé€šå¸¸ï¼‰
        best_match = None
        best_ratio = 0

        for db_name in self.talent_map.keys():
            # å…ƒã®åå‰ã§ã®ãƒãƒƒãƒãƒ³ã‚°
            ratio1 = SequenceMatcher(None, csv_name, db_name).ratio()
            # æ­£è¦åŒ–åã§ã®ãƒãƒƒãƒãƒ³ã‚°
            ratio2 = SequenceMatcher(None, normalized_csv, self.normalize_name(db_name)).ratio()

            # ã‚ˆã‚Šé«˜ã„ã‚¹ã‚³ã‚¢ã‚’æ¡ç”¨
            ratio = max(ratio1, ratio2)

            if ratio > best_ratio and ratio >= threshold:
                best_ratio = ratio
                best_match = db_name

        if best_match:
            return self.talent_map[best_match], f"fuzzy_{best_ratio:.2f}"

        return None, "no_match"

    async def process_csv_file(self, csv_file, target_segment_id, dry_run=True):
        """å˜ä¸€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
        logger.info(f"ğŸ“ Processing: {csv_file.name}")

        try:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆUTF-8 BOMå¯¾å¿œï¼‰
            df = pd.read_csv(csv_file, encoding='utf-8-sig')

            # å¿…é ˆã‚«ãƒ©ãƒ ã®ç¢ºèª
            required_cols = ['ã‚¿ãƒ¬ãƒ³ãƒˆå', 'ã‚¹ã‚³ã‚¢']
            if not all(col in df.columns for col in required_cols):
                logger.error(f"âŒ Required columns missing: {required_cols}")
                return 0

            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            df = df.dropna(subset=['ã‚¿ãƒ¬ãƒ³ãƒˆå', 'ã‚¹ã‚³ã‚¢'])
            df['ã‚¿ãƒ¬ãƒ³ãƒˆå'] = df['ã‚¿ãƒ¬ãƒ³ãƒˆå'].astype(str).str.strip()
            df['ã‚¹ã‚³ã‚¢'] = pd.to_numeric(df['ã‚¹ã‚³ã‚¢'], errors='coerce')
            df = df.dropna(subset=['ã‚¹ã‚³ã‚¢'])

            logger.info(f"ğŸ“Š Total records in CSV: {len(df)}")

            updated_records = []

            for idx, row in df.iterrows():
                talent_name = row['ã‚¿ãƒ¬ãƒ³ãƒˆå']
                power_score = float(row['ã‚¹ã‚³ã‚¢'])

                # åå‰ãƒãƒƒãƒãƒ³ã‚°
                account_id, match_type = self.find_best_match(talent_name)

                if account_id:
                    self.matched_count += 1

                    if match_type.startswith("fuzzy"):
                        self.fuzzy_matches.append({
                            'csv_name': talent_name,
                            'db_name': [name for name, aid in self.talent_map.items() if aid == account_id][0],
                            'account_id': account_id,
                            'match_ratio': match_type,
                            'power_score': power_score,
                            'target_segment_id': target_segment_id,
                            'file': csv_file.name
                        })

                    updated_records.append({
                        'account_id': account_id,
                        'target_segment_id': target_segment_id,
                        'tpr_power_score': power_score,
                        'csv_name': talent_name,
                        'match_type': match_type
                    })

                else:
                    self.unmatched_count += 1
                    self.unmatched_list.append({
                        'csv_name': talent_name,
                        'csv_kana': row.get('ã‚¿ãƒ¬ãƒ³ãƒˆå(å…¨è§’ã‚«ãƒŠ)', ''),
                        'power_score': power_score,
                        'file': csv_file.name,
                        'target_segment': target_segment_id
                    })

            if not dry_run and updated_records:
                await self.update_database(updated_records, target_segment_id)

            logger.info(f"âœ… {csv_file.name}: {len(updated_records)} matched, {len(df) - len(updated_records)} unmatched")
            return len(updated_records)

        except Exception as e:
            logger.error(f"âŒ Error processing {csv_file.name}: {e}")
            return 0

    async def update_database(self, records, target_segment_id):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ï¼ˆç”ŸSQLä½¿ç”¨ï¼‰"""
        async with get_session_maker()() as session:
            try:
                for record in records:
                    account_id = record['account_id']
                    segment_id = record['target_segment_id']
                    tpr_score = Decimal(str(record['tpr_power_score']))

                    # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºèªï¼ˆç”ŸSQLã‚¯ã‚¨ãƒªï¼‰
                    result = await session.execute(
                        text('''
                            SELECT account_id, vr_popularity, tpr_power_score
                            FROM talent_scores
                            WHERE account_id = :account_id
                              AND target_segment_id = :target_segment_id
                        '''),
                        {
                            'account_id': account_id,
                            'target_segment_id': segment_id
                        }
                    )
                    existing_record = result.fetchone()

                    if existing_record:
                        # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰æ›´æ–°
                        vr_val = existing_record[1] if existing_record[1] else Decimal('0')
                        new_base_power = (vr_val + tpr_score) / 2

                        await session.execute(
                            text('''
                                UPDATE talent_scores
                                SET tpr_power_score = :tpr_score,
                                    base_power_score = :base_power_score,
                                    updated_at = CURRENT_TIMESTAMP
                                WHERE account_id = :account_id
                                  AND target_segment_id = :target_segment_id
                            '''),
                            {
                                'account_id': account_id,
                                'target_segment_id': segment_id,
                                'tpr_score': tpr_score,
                                'base_power_score': new_base_power
                            }
                        )

                    else:
                        # æ–°è¦ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆï¼ˆVRãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼‰
                        new_base_power = (Decimal('0') + tpr_score) / 2

                        await session.execute(
                            text('''
                                INSERT INTO talent_scores
                                (account_id, target_segment_id, vr_popularity, tpr_power_score, base_power_score, created_at, updated_at)
                                VALUES (:account_id, :target_segment_id, NULL, :tpr_score, :base_power_score, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                            '''),
                            {
                                'account_id': account_id,
                                'target_segment_id': segment_id,
                                'tpr_score': tpr_score,
                                'base_power_score': new_base_power
                            }
                        )

                await session.commit()
                logger.info(f"âœ… Database updated: {len(records)} records")

            except Exception as e:
                await session.rollback()
                logger.error(f"âŒ Database update failed: {e}")
                raise

    def generate_reports(self):
        """ãƒãƒƒãƒãƒ³ã‚°çµæœãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # 1. ãƒãƒƒãƒãƒ³ã‚°å¤±æ•—ãƒ¬ãƒãƒ¼ãƒˆ
        if self.unmatched_list:
            df_unmatched = pd.DataFrame(self.unmatched_list)
            unmatched_file = f"tpr_unmatched_{timestamp}.csv"
            df_unmatched.to_csv(unmatched_file, index=False, encoding='utf-8-sig')
            logger.warning(f"ğŸ“„ Unmatched names report: {unmatched_file}")

            # ä¸Šä½10ä»¶ã‚’è¡¨ç¤º
            logger.warning(f"âš ï¸ Top 10 unmatched names:")
            for i, item in enumerate(self.unmatched_list[:10], 1):
                logger.warning(f"   {i}. {item['csv_name']} (score: {item['power_score']}, file: {item['file']})")

        # 2. ã‚ã„ã¾ã„ãƒãƒƒãƒãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ
        if self.fuzzy_matches:
            df_fuzzy = pd.DataFrame(self.fuzzy_matches)
            fuzzy_file = f"tpr_fuzzy_matches_{timestamp}.csv"
            df_fuzzy.to_csv(fuzzy_file, index=False, encoding='utf-8-sig')
            logger.info(f"ğŸ“„ Fuzzy matches report: {fuzzy_file}")

            # ä¸Šä½5ä»¶ã‚’è¡¨ç¤º
            logger.info(f"ğŸ” Top 5 fuzzy matches:")
            for i, item in enumerate(self.fuzzy_matches[:5], 1):
                logger.info(f"   {i}. '{item['csv_name']}' â†’ '{item['db_name']}' ({item['match_ratio']})")

        # 3. ã‚µãƒãƒªãƒ¼
        total_processed = self.matched_count + self.unmatched_count
        match_rate = (self.matched_count / total_processed * 100) if total_processed > 0 else 0

        logger.info(f"\nğŸ“Š Processing Summary:")
        logger.info(f"   Total processed: {total_processed}")
        logger.info(f"   Matched: {self.matched_count} ({match_rate:.1f}%)")
        logger.info(f"   Unmatched: {self.unmatched_count} ({100-match_rate:.1f}%)")
        logger.info(f"   Fuzzy matches: {len(self.fuzzy_matches)}")

    async def run(self, dry_run=True):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œå‡¦ç†"""
        logger.info("=" * 80)
        if dry_run:
            logger.info("ğŸ§ª TPR DATA UPDATE - DRY RUN MODE")
        else:
            logger.info("ğŸš€ TPR DATA UPDATE - EXECUTE MODE")
        logger.info("=" * 80)

        await init_db()
        await self.load_talent_mapping()

        total_updated = 0

        # å„TPRãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        for filename, target_segment_id in TPR_FILES_MAPPING.items():
            csv_file = CSV_DIR / filename

            if not csv_file.exists():
                logger.warning(f"âš ï¸ File not found: {filename}")
                continue

            updated_count = await self.process_csv_file(csv_file, target_segment_id, dry_run)
            total_updated += updated_count

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_reports()

        if dry_run:
            logger.info(f"\nğŸ§ª DRY RUN COMPLETE: {total_updated} records would be updated")
        else:
            logger.info(f"\nâœ… UPDATE COMPLETE: {total_updated} records updated")

        logger.info("=" * 80)

        return total_updated


async def main():
    parser = argparse.ArgumentParser(
        description='TPRãƒ‘ãƒ¯ãƒ¼ã‚¹ã‚³ã‚¢æ›´æ–°ï¼ˆåå‰ãƒãƒƒãƒãƒ³ã‚°å¯¾å¿œç‰ˆï¼‰'
    )
    parser.add_argument('--dry-run', action='store_true', help='ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆæ¤œè¨¼ã®ã¿ï¼‰')
    parser.add_argument('--execute', action='store_true', help='å®Ÿéš›ã«æ›´æ–°å®Ÿè¡Œ')

    args = parser.parse_args()

    if not (args.dry_run or args.execute):
        logger.error("âŒ --dry-run ã¾ãŸã¯ --execute ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        return

    if args.execute:
        print("\nâš ï¸ å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã—ã¾ã™ã€‚")
        confirm = input("ç¶™ç¶šã—ã¾ã™ã‹ï¼Ÿ (yes/no): ")
        if confirm.lower() != 'yes':
            logger.info("âŒ å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            return

    importer = TPRImporter()
    await importer.run(dry_run=args.dry_run)


if __name__ == "__main__":
    asyncio.run(main())