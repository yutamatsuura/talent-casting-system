#!/usr/bin/env python3
"""
ã‚¿ãƒ¬ãƒ³ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  åŠ¹ç‡çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ”¯æ´ãƒ„ãƒ¼ãƒ«
"""

import asyncio
import time
import json
import statistics
from datetime import datetime
from typing import Dict, List, Tuple, Any
import logging
from dataclasses import dataclass
import psutil
import requests

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TestCase:
    """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©"""
    name: str
    company_name: str
    industry: str
    target_segments: str
    purpose: str
    budget: str
    email: str
    expected_min_results: int = 20

@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    response_time: float
    cpu_usage_before: float
    cpu_usage_after: float
    memory_usage_before: float
    memory_usage_after: float
    result_count: int
    top_3_talents: List[Dict]

class TuningTestSuite:
    """ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ"""

    def __init__(self, api_base_url: str = "http://localhost:8432"):
        self.api_base_url = api_base_url
        self.test_cases = self._define_test_cases()

    def _define_test_cases(self) -> List[TestCase]:
        """ä»£è¡¨çš„ãªãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®šç¾©"""
        return [
            # é«˜è² è·ã‚±ãƒ¼ã‚¹ï¼šäººæ°—æ¥­ç•Œ Ã— äººæ°—ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            TestCase(
                name="é«˜è² è·_åŒ–ç²§å“_å¥³æ€§20-34",
                company_name="æ ªå¼ä¼šç¤¾ãƒ†ã‚¹ãƒˆãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",
                industry="åŒ–ç²§å“ãƒ»ãƒ˜ã‚¢ã‚±ã‚¢ãƒ»ã‚ªãƒ¼ãƒ©ãƒ«ã‚±ã‚¢",
                target_segments="å¥³æ€§20-34æ­³",
                purpose="ãƒ–ãƒ©ãƒ³ãƒ‰ã®èªçŸ¥åº¦å‘ä¸Šã®ãŸã‚",
                budget="3,000ä¸‡å††ã€œ1å„„å††æœªæº€",
                email="test@beauty-test.com"
            ),

            # ä¸­è² è·ã‚±ãƒ¼ã‚¹ï¼šä¸€èˆ¬çš„ãªçµ„ã¿åˆã‚ã›
            TestCase(
                name="ä¸­è² è·_é£Ÿå“_å¥³æ€§35-49",
                company_name="æ ªå¼ä¼šç¤¾ãƒ†ã‚¹ãƒˆãƒ•ãƒ¼ãƒ‰",
                industry="é£Ÿå“",
                target_segments="å¥³æ€§35-49æ­³",
                purpose="æ–°å•†å“ã®ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚",
                budget="1,000ä¸‡å††ã€œ3,000ä¸‡å††æœªæº€",
                email="test@food-test.com"
            ),

            # ä½è² è·ã‚±ãƒ¼ã‚¹ï¼šãƒ‹ãƒƒãƒãªçµ„ã¿åˆã‚ã›
            TestCase(
                name="ä½è² è·_é‡‘è_ç”·æ€§50-69",
                company_name="æ ªå¼ä¼šç¤¾ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹",
                industry="é‡‘èãƒ»ä¸å‹•ç”£",
                target_segments="ç”·æ€§50-69æ­³",
                purpose="ä¿¡é ¼æ€§å‘ä¸Šã®ãŸã‚",
                budget="1,000ä¸‡å††æœªæº€",
                email="test@finance-test.com"
            ),

            # æ¥µé™ã‚±ãƒ¼ã‚¹ï¼šè¤‡æ•°ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            TestCase(
                name="è¤‡é›‘_è‡ªå‹•è»Š_è¤‡æ•°ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ",
                company_name="æ ªå¼ä¼šç¤¾ãƒ†ã‚¹ãƒˆã‚ªãƒ¼ãƒˆ",
                industry="è‡ªå‹•è»Šé–¢é€£",
                target_segments="ç”·æ€§35-49æ­³",
                purpose="ãƒ–ãƒ©ãƒ³ãƒ‰ã‚¤ãƒ¡ãƒ¼ã‚¸å‘ä¸Šã®ãŸã‚",
                budget="1å„„å††ä»¥ä¸Š",
                email="test@auto-test.com"
            )
        ]

    async def run_single_test(self, test_case: TestCase) -> PerformanceMetrics:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®å®Ÿè¡Œ"""
        logger.info(f"ğŸ§ª ãƒ†ã‚¹ãƒˆé–‹å§‹: {test_case.name}")

        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æ¸¬å®šï¼ˆé–‹å§‹å‰ï¼‰
        cpu_before = psutil.cpu_percent(interval=0.1)
        memory_before = psutil.virtual_memory().percent

        # APIå‘¼ã³å‡ºã—
        start_time = time.time()

        payload = {
            "company_name": test_case.company_name,
            "industry": test_case.industry,
            "target_segments": test_case.target_segments,
            "purpose": test_case.purpose,
            "budget": test_case.budget,
            "email": test_case.email
        }

        try:
            response = requests.post(
                f"{self.api_base_url}/api/matching",
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            result_data = response.json()

            end_time = time.time()
            response_time = end_time - start_time

            # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æ¸¬å®šï¼ˆçµ‚äº†å¾Œï¼‰
            cpu_after = psutil.cpu_percent(interval=0.1)
            memory_after = psutil.virtual_memory().percent

            # çµæœè§£æ
            results = result_data.get('results', [])
            top_3_talents = results[:3] if results else []

            metrics = PerformanceMetrics(
                response_time=response_time,
                cpu_usage_before=cpu_before,
                cpu_usage_after=cpu_after,
                memory_usage_before=memory_before,
                memory_usage_after=memory_after,
                result_count=len(results),
                top_3_talents=top_3_talents
            )

            logger.info(f"âœ… {test_case.name}: {response_time:.3f}ç§’, {len(results)}ä»¶")
            return metrics

        except Exception as e:
            logger.error(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ {test_case.name}: {str(e)}")
            raise

    async def run_benchmark_suite(self, iterations: int = 5) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ"""
        logger.info(f"ğŸš€ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹ ({iterations}å›å®Ÿè¡Œ)")

        all_results = {}

        for test_case in self.test_cases:
            logger.info(f"ğŸ“Š {test_case.name} ã‚’ {iterations}å›å®Ÿè¡Œä¸­...")

            metrics_list = []
            for i in range(iterations):
                try:
                    await asyncio.sleep(1)  # APIè² è·è»½æ¸›
                    metrics = await self.run_single_test(test_case)
                    metrics_list.append(metrics)
                    logger.info(f"  è©¦è¡Œ {i+1}/{iterations}: {metrics.response_time:.3f}ç§’")
                except Exception as e:
                    logger.warning(f"  è©¦è¡Œ {i+1}/{iterations} å¤±æ•—: {str(e)}")
                    continue

            if metrics_list:
                # çµ±è¨ˆè¨ˆç®—
                response_times = [m.response_time for m in metrics_list]
                result_counts = [m.result_count for m in metrics_list]

                all_results[test_case.name] = {
                    "test_case": test_case.__dict__,
                    "iterations": len(metrics_list),
                    "performance": {
                        "avg_response_time": statistics.mean(response_times),
                        "min_response_time": min(response_times),
                        "max_response_time": max(response_times),
                        "std_dev": statistics.stdev(response_times) if len(response_times) > 1 else 0,
                        "percentile_95": sorted(response_times)[int(len(response_times) * 0.95)] if response_times else 0
                    },
                    "consistency": {
                        "avg_result_count": statistics.mean(result_counts),
                        "result_count_variance": statistics.variance(result_counts) if len(result_counts) > 1 else 0,
                        "top_talent_consistency": self._check_consistency([m.top_3_talents for m in metrics_list])
                    },
                    "raw_metrics": [
                        {
                            "response_time": m.response_time,
                            "result_count": m.result_count,
                            "cpu_usage_delta": m.cpu_usage_after - m.cpu_usage_before,
                            "memory_usage_delta": m.memory_usage_after - m.memory_usage_before
                        } for m in metrics_list
                    ]
                }

        # ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        benchmark_report = {
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "suite_type": "comprehensive_benchmark",
            "total_tests": len(self.test_cases),
            "total_iterations": sum(result.get("iterations", 0) for result in all_results.values()),
            "results": all_results,
            "summary": self._generate_summary(all_results),
            "recommendations": self._generate_recommendations(all_results)
        }

        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        output_file = f"benchmark_comprehensive_{benchmark_report['timestamp']}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(benchmark_report, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“‹ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†: {output_file}")
        return benchmark_report

    def _check_consistency(self, top_talents_list: List[List[Dict]]) -> Dict[str, Any]:
        """ãƒˆãƒƒãƒ—ã‚¿ãƒ¬ãƒ³ãƒˆä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯"""
        if not top_talents_list:
            return {"consistency_score": 0, "note": "ãƒ‡ãƒ¼ã‚¿ãªã—"}

        # 1ä½ã‚¿ãƒ¬ãƒ³ãƒˆã®ä¸€è²«æ€§
        first_place_talents = [talents[0]['name'] if talents else None for talents in top_talents_list]
        first_place_consistency = len(set(filter(None, first_place_talents))) <= 2

        # TOP3å¹³å‡ä¸€è²«æ€§
        all_top3_names = []
        for talents in top_talents_list:
            all_top3_names.extend([t['name'] for t in talents[:3]])

        unique_top3 = len(set(all_top3_names))
        total_appearances = len(all_top3_names)

        return {
            "first_place_consistent": first_place_consistency,
            "unique_top3_talents": unique_top3,
            "total_top3_appearances": total_appearances,
            "consistency_score": (total_appearances - unique_top3) / max(total_appearances, 1) * 100
        }

    def _generate_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """çµæœã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        all_avg_times = [result["performance"]["avg_response_time"] for result in results.values()]

        return {
            "overall_avg_response_time": statistics.mean(all_avg_times) if all_avg_times else 0,
            "fastest_test_case": min(results.keys(), key=lambda x: results[x]["performance"]["avg_response_time"]) if results else None,
            "slowest_test_case": max(results.keys(), key=lambda x: results[x]["performance"]["avg_response_time"]) if results else None,
            "performance_variance": statistics.variance(all_avg_times) if len(all_avg_times) > 1 else 0,
            "total_execution_time": sum(result["performance"]["avg_response_time"] * result["iterations"] for result in results.values())
        }

    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        avg_times = [result["performance"]["avg_response_time"] for result in results.values()]
        if avg_times:
            max_time = max(avg_times)
            if max_time > 5.0:
                recommendations.append("âš ï¸ 5ç§’è¶…éã®ã‚±ãƒ¼ã‚¹ã‚ã‚Š - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–æ¤œè¨")
            elif max_time > 3.0:
                recommendations.append("ğŸ“ˆ 3ç§’è¶…éã®ã‚±ãƒ¼ã‚¹ã‚ã‚Š - ã‚¯ã‚¨ãƒªæœ€é©åŒ–æ¤œè¨")
            elif max_time < 1.0:
                recommendations.append("ğŸš€ å„ªç§€ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ - ç¾çŠ¶ç¶­æŒæ¨å¥¨")

        # ä¸€è²«æ€§åˆ†æ
        consistency_scores = [
            result["consistency"].get("consistency_score", 0)
            for result in results.values()
        ]
        if consistency_scores:
            avg_consistency = statistics.mean(consistency_scores)
            if avg_consistency < 80:
                recommendations.append("ğŸ”„ çµæœä¸€è²«æ€§ä½ä¸‹ - ãƒãƒƒãƒãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ç¢ºèªå¿…è¦")
            elif avg_consistency > 95:
                recommendations.append("âœ… é«˜ã„çµæœä¸€è²«æ€§ - ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®‰å®š")

        # ãƒãƒªã‚¢ãƒ³ã‚¹åˆ†æ
        for test_name, result in results.items():
            std_dev = result["performance"]["std_dev"]
            avg_time = result["performance"]["avg_response_time"]
            if avg_time > 0 and (std_dev / avg_time) > 0.3:
                recommendations.append(f"ğŸ“Š {test_name}: å®Ÿè¡Œæ™‚é–“ã®ã°ã‚‰ã¤ãå¤§ - è² è·åˆ†æ•£æ¤œè¨")

        return recommendations

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ ã‚¿ãƒ¬ãƒ³ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡åŒ–ãƒ„ãƒ¼ãƒ«")
    print("=" * 60)

    # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆåˆæœŸåŒ–
    suite = TuningTestSuite()

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    try:
        report = await suite.run_benchmark_suite(iterations=3)

        # çµæœè¡¨ç¤º
        print("\nğŸ“Š å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼:")
        print("-" * 40)
        summary = report["summary"]
        print(f"å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {summary['overall_avg_response_time']:.3f}ç§’")
        print(f"æœ€é€Ÿã‚±ãƒ¼ã‚¹: {summary['fastest_test_case']}")
        print(f"æœ€é…ã‚±ãƒ¼ã‚¹: {summary['slowest_test_case']}")

        print("\nğŸ’¡ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¨å¥¨äº‹é …:")
        print("-" * 40)
        for rec in report["recommendations"]:
            print(f"  {rec}")

        print(f"\nğŸ“‹ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: benchmark_comprehensive_{report['timestamp']}.json")

    except Exception as e:
        logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")

if __name__ == "__main__":
    asyncio.run(main())